{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_LAB3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlXY7CAbzEANZ8PFymv+LE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikunjbansal99/NLP_Lab/blob/main/NLP_LAB3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVqGKG9aYZBl"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvh-hjAlQiXS"
      },
      "source": [
        "**Word Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i47A29cNpsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "81758a8c-53ce-429a-d29d-87287ae29afe"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k409BOVKn1A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c35b21e-60d1-4329-c76d-5a5f16e4ee7a"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "input = \"Have a nice day\"\n",
        "print(word_tokenize(input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Have', 'a', 'nice', 'day']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0X_Q2hc5n1"
      },
      "source": [
        "**Treebank Word Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bSS1ys1YFBX"
      },
      "source": [
        "These tokenizers work by separating the words using punctuation and spaces. And as mentioned in the code outputs above, it does not discard the punctuation, allowing a user to decide what to do with the punctuations at the time of pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J-JM0MXc4c-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "810b2c8f-0cae-4ed9-c9d5-3072253ebad0"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer \n",
        "input = \"It's amazing. It's excited.\"\n",
        "t = TreebankWordTokenizer() \n",
        "t.tokenize(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It', \"'s\", 'amazing.', 'It', \"'s\", 'excited', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCD8nxztgOvV"
      },
      "source": [
        "**WordPunct Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcia0JKRYCrh"
      },
      "source": [
        " It seperates the punctuation from the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btDSLdaphP_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09444181-0d67-4fca-aaa8-74fd07e72677"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer \n",
        "input = \"It's amazing. It's excited.\"\n",
        "t = WordPunctTokenizer() \n",
        "t.tokenize(input) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It', \"'\", 's', 'amazing', '.', 'It', \"'\", 's', 'excited', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKmL_FPfQl00"
      },
      "source": [
        "**Sentence Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRf1bNjhNsZ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a554ccac-d6ab-484e-9b3f-0c7b2db14ec9"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "input = \"It's amazing. It's excited.\"\n",
        "print(sent_tokenize(input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"It's amazing.\", \"It's excited.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mycMAc-YZVCG"
      },
      "source": [
        "**Tokenize using Regular Expression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYMlW39xVrss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7893ffe4-57c6-471d-9e3f-f4c82e012eb3"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer \n",
        "t = RegexpTokenizer('\\s+', gaps = True)\n",
        "input = \"Maybe I am the happiest person\"\n",
        "tokens = t.tokenize(input) \n",
        "print(tokens) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Maybe', 'I', 'am', 'the', 'happiest', 'person']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt4XcqBXpVJP"
      },
      "source": [
        "**Whitespace Tokenizer** <br>\n",
        "Tokenize a string on whitespace (space, tab, newline). In general, users should use the string split() method instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bGsMPEfpTm4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d1db297-54fb-4ae7-d534-14ac03de5302"
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "input = \"It's amazing.\\n It's \\texcited.\"\n",
        "t = WhitespaceTokenizer() \n",
        "t.tokenize(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"It's\", 'amazing.', \"It's\", 'excited.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TRY_IgcSVeZ"
      },
      "source": [
        "**Blankline Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdV2SUEVonNE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11134882-eba0-4f53-f748-d0ebb68035ee"
      },
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "input = \"It's amazing.\\n It's \\texcited.\"\n",
        "t = BlanklineTokenizer()\n",
        "t.tokenize(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"It's amazing.\\n It's \\texcited.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYMK2jQPllmE"
      },
      "source": [
        "**Line Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLgZaoaCp5Kk"
      },
      "source": [
        "Tokenize a string into its lines, optionally discarding blank lines. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlVtzlS_lw9p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ccf05636-6d78-4386-8d5e-5cad01750160"
      },
      "source": [
        "from nltk.tokenize import LineTokenizer\n",
        "input = \"Hello \\nI am Nikunj Bansal. Student Now\\nML Researcher in upcoming Future.\\n\\nThanks.\"\n",
        "t = LineTokenizer(blanklines='keep')  #keep  blank lines\n",
        "t.tokenize(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello ',\n",
              " 'I am Nikunj Bansal. Student Now',\n",
              " 'ML Researcher in upcoming Future.',\n",
              " '',\n",
              " 'Thanks.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeVRQtnPqbC2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "92488cae-7086-4349-ec9e-e5ded8eb0e64"
      },
      "source": [
        "t = LineTokenizer(blanklines='discard')  #discard  blank lines\n",
        "t.tokenize(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello ',\n",
              " 'I am Nikunj Bansal. Student Now',\n",
              " 'ML Researcher in upcoming Future.',\n",
              " 'Thanks.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xXJog_ZrM8v"
      },
      "source": [
        "**String Span Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XmU7OMKrv_R"
      },
      "source": [
        "Return the offsets of the tokens in s, as a sequence of (start, end) tuples, by splitting the string at each occurrence of sep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plg4OlbQrNLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3467dae2-1d1f-44f0-da6a-cf6d1c30a940"
      },
      "source": [
        "from nltk.tokenize.util import string_span_tokenize\n",
        "input = \"Hello \\nI am Nikunj Bansal. Student Now\\nML Researcher in upcoming Future.\\n\\nThanks.\"\n",
        "list(string_span_tokenize(input, \" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 5),\n",
              " (6, 8),\n",
              " (9, 11),\n",
              " (12, 18),\n",
              " (19, 26),\n",
              " (27, 34),\n",
              " (35, 41),\n",
              " (42, 52),\n",
              " (53, 55),\n",
              " (56, 64),\n",
              " (65, 81)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ufxTNVrm5Kd"
      },
      "source": [
        "**Tweet Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nutIZbqtnOB3"
      },
      "source": [
        "Twitter-aware tokenizer, designed to be flexible and easy to adapt to new domains and tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn4XR4Uam4L0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b068b9dd-14c0-4f87-97a9-87556ee2b8a3"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "t = TweetTokenizer()\n",
        "input = \"It's amazing: :-) :-P <3 It's excited.$$--->:-)\"\n",
        "t.tokenize(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"It's\",\n",
              " 'amazing',\n",
              " ':',\n",
              " ':-)',\n",
              " ':-P',\n",
              " '<3',\n",
              " \"It's\",\n",
              " 'excited',\n",
              " '.',\n",
              " '$',\n",
              " '$',\n",
              " '--->',\n",
              " ':-)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNIQ6k1nnmFm"
      },
      "source": [
        "**Multi-Word Expression Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFB1XMGBnuK5"
      },
      "source": [
        "A MWETokenizer takes a string which has already been divided into tokens and retokenizes it, merging multi-word expressions into single tokens, using a lexicon of MWEs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCz7FbUxnItR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e57e7a20-8518-4613-df9b-346695865bb4"
      },
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "t = MWETokenizer([('Nikunj', 'Bansal'), ('Roll', 'No.', '63'), ('Batch', 'B2')])\n",
        "t.add_mwe(('SAP', 'Id', '500069944'))\n",
        "t.tokenize('I am Nikunj Bansal having a Roll No. 63 and SAP Id 500069944 of Batch B2'.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'Nikunj_Bansal',\n",
              " 'having',\n",
              " 'a',\n",
              " 'Roll_No._63',\n",
              " 'and',\n",
              " 'SAP_Id_500069944',\n",
              " 'of',\n",
              " 'Batch_B2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}